{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferring the actitude toward self-driving cars from tweets using an NN-based supervised classification algorithm\n",
    "\n",
    "#### Objective: infer the reactions of users to self-driving cars from their tweets\n",
    "#### DataSet: the data set is provided by kaggle and can be found in the this link:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this approach we will\n",
    "    \n",
    "    •Preprocess the data\n",
    "        - removing  extrange characters \n",
    "        - parsing html formated strings\n",
    "        - removing links and #,@\n",
    "        - expanding contractions \n",
    "        \n",
    "#### Main approach:\n",
    "    •feature extraction (tfidf vectorizer)\n",
    "    •Train our model (in this case a Multilayer neural network)\n",
    "    •Test the MNN\n",
    "    \n",
    "#### Secondary Approaches\n",
    "   **CNN**\n",
    "  \n",
    "    •feature extraction (word2vec)\n",
    "    •Train a CNN \n",
    "    •Test the CNN\n",
    "    \n",
    "   **LSTM**\n",
    "   \n",
    "    •feature extraction (also a TFIDF variation)\n",
    "    •Train a LSTM net\n",
    "    •Test the LSTM net\n",
    "    \n",
    " And Finally comparation between reasults and embeddings \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important! before running this notebook please configure the enviroment Running the following cell\n",
    "\n",
    "#### Better to use in linux/unix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!conda env create -f enviroment.yml\\n!source activate sentimentAnalysis\\n#And then reload the notebook doing\\n!jupyter notebook P38_TwitterSelfDrivingCars.ipynb'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!conda env create -f enviroment.yml\n",
    "!source activate sentimentAnalysis\n",
    "#And then reload the notebook doing\n",
    "!jupyter notebook P38_TwitterSelfDrivingCars.ipynb\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data\n",
    "\n",
    "##### Downloading the dataset from https://www.kaggle.com/c/self-driving-cars-sentiment-analysis/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "testing_data=pd.read_csv(\"test_data.csv\")\n",
    "training_data=pd.read_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>How is an automated vehicle supposed to behave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>#AnnArbor is home to ongoing experiment w/ tal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>WO2012158248A1 Collaborative Vehicle Control U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>Wow - \"@AUVSI: Google goes commercial: Company...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>Blogged: Automated vehicles and the Motor Vehi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>California's new automated vehicle regulations...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>Last year, Google warned it would have an auto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>@ford video: sensors on Fusion Hybrid Automate...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                               text\n",
       "37          3  How is an automated vehicle supposed to behave...\n",
       "38          3  #AnnArbor is home to ongoing experiment w/ tal...\n",
       "39          3  WO2012158248A1 Collaborative Vehicle Control U...\n",
       "40          3  Wow - \"@AUVSI: Google goes commercial: Company...\n",
       "41          3  Blogged: Automated vehicles and the Motor Vehi...\n",
       "42          3  California's new automated vehicle regulations...\n",
       "43          3  Last year, Google warned it would have an auto...\n",
       "44          3  @ford video: sensors on Fusion Hybrid Automate..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[37:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data hasent being converted to text because there is some weird characters,\"hashtags\" and mentions in the tweets that could diminish the perfomance of the prediction, the first steps in the cleaning will be \n",
    "\n",
    "    - Parsing html to text\n",
    "    - Remove links \n",
    "    - Remove \"@\" from mentions (I decided not to remove mentions because it carryes some information) and # from hashtags??\n",
    "    - Remove weird Characters\n",
    "    - Removing punctuations.\n",
    "    \n",
    "so lets define a cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import string\n",
    "\n",
    "contractions_dict = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"w/\": \"with\"\n",
    "}\n",
    "\n",
    "contractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "def expand_contractions(s, contractions_dict=contractions_dict):\n",
    "    return contractions_re.sub(replace, s)\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "remove_punctuation=str.maketrans('','',string.punctuation)\n",
    "removeLinks=re.compile(\"https?://[A-Za-z0-9./]+\")\n",
    "\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    #parsing html\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    souped = soup.get_text()\n",
    "    #removing links and @ \n",
    "    stripped = removeLinks.sub('', souped)\n",
    "    #removing weird characters\n",
    "    clean = stripped.encode(\"utf-8-sig\").decode(\"ascii\",\"ignore\")\n",
    "    #expanding contractions \n",
    "    clean=expand_contractions(clean)\n",
    "    #removing punctuation\n",
    "    clean=clean.translate(remove_punctuation)\n",
    "    lower_case = clean.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    #words = tok.tokenize(clean)\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Whit the cleaning function designed now is possible to preprocess the text in order to increase perfomance and diminish ambiguity \n",
    "\n",
    "i.e lets clean the following samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=training_data.text[14]\n",
    "example2=training_data.text[38]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ̢���@latimesautos: Audi gets first permit to test self-driving cars on California roads: http://t.co/L71qpM42lY http://t.co/CWBbOAwahL̢���� ̡�����_̡�����\n",
      "\n",
      "Cleaned:latimesautos audi gets first permit to test selfdriving cars on california roads\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {example}\\n\\nCleaned:{tweet_cleaner(example)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: #AnnArbor is home to ongoing experiment w/ talking #cars? \"@freep: Automated vehicle test site groundbreaking today http://t.co/QwbtJbSBwy\"\n",
      "\n",
      "Cleaned: annarbor is home to ongoing experiment with talking cars freep automated vehicle test site groundbreaking today\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original: {example2}\\n\\nCleaned: {tweet_cleaner(example2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Now its time to clean the data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "1000 Tweets of 5250 has been processed\n",
      "2000 Tweets of 5250 has been processed\n",
      "3000 Tweets of 5250 has been processed\n",
      "4000 Tweets of 5250 has been processed\n",
      "5000 Tweets of 5250 has been processed\n",
      "Data Cleaned!!\n"
     ]
    }
   ],
   "source": [
    "print (\"Cleaning and parsing the tweets...\\n\")\n",
    "for i in range(0,training_data.shape[0]):\n",
    "    if( (i+1)%1000 == 0 ):\n",
    "        print(f\"{i+1} Tweets of {training_data.shape[0]} has been processed\")                                                                 \n",
    "    training_data.at[i,\"text\"]=tweet_cleaner(training_data.text[i])\n",
    "print(\"Data Cleaned!!\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>how is an automated vehicle supposed to behave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>annarbor is home to ongoing experiment with ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>wo2012158248a1 collaborative vehicle control u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>wow auvsi google goes commercial company teams...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>blogged automated vehicles and the motor vehic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>californias new automated vehicle regulations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>last year google warned it would have an autom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>ford video sensors on fusion hybrid automated ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                               text\n",
       "37          3  how is an automated vehicle supposed to behave...\n",
       "38          3  annarbor is home to ongoing experiment with ta...\n",
       "39          3  wo2012158248a1 collaborative vehicle control u...\n",
       "40          3  wow auvsi google goes commercial company teams...\n",
       "41          3  blogged automated vehicles and the motor vehic...\n",
       "42          3  californias new automated vehicle regulations ...\n",
       "43          3  last year google warned it would have an autom...\n",
       "44          3  ford video sensors on fusion hybrid automated ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[37:45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**I store the cleaned data set for futher uses**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv(\"cleaned_trainig_data.csv\",encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Also the testing data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and parsing the tweets...\n",
      "\n",
      "200 Tweets of 1693 has been processed\n",
      "400 Tweets of 1693 has been processed\n",
      "600 Tweets of 1693 has been processed\n",
      "800 Tweets of 1693 has been processed\n",
      "1000 Tweets of 1693 has been processed\n",
      "1200 Tweets of 1693 has been processed\n",
      "1400 Tweets of 1693 has been processed\n",
      "1600 Tweets of 1693 has been processed\n",
      "Data Cleaned!!\n"
     ]
    }
   ],
   "source": [
    "print (\"Cleaning and parsing the tweets...\\n\")\n",
    "for i in range(0,testing_data.shape[0]):\n",
    "    if( (i+1)%200 == 0 ):\n",
    "        print(f\"{i+1} Tweets of {testing_data.shape[0]} has been processed\")                                                                 \n",
    "    testing_data.at[i,\"text\"]=tweet_cleaner(testing_data.text[i])\n",
    "print(\"Data Cleaned!!\")\n",
    "testing_data.to_csv(\"cleaned_testing_data.csv\",encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The next step is just a comprobation it can be skipped***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"cleaned_trainig_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>two places id invest all my money if i could 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>awesome google driverless cars will help the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>if google maps cannot keep up with road constr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>autonomous cars seem way overhyped given the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>just saw google selfdriving car on i34 it was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          5  two places id invest all my money if i could 3...\n",
       "1          5  awesome google driverless cars will help the b...\n",
       "2          2  if google maps cannot keep up with road constr...\n",
       "3          2  autonomous cars seem way overhyped given the t...\n",
       "4          3  just saw google selfdriving car on i34 it was ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data representation (feature extraction tfidf and wordvectorizer)\n",
    "\n",
    "### Here we start with the main approach\n",
    "For this time we will use a tfidf vectorizer which s a word embedding that could represent not only the amount of word but also reflect how important a word is to a document in a collection\n",
    "\n",
    "then i will use wordvectorizer that will count the ocurrency of the words in a given document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cvec = CountVectorizer()\n",
    "tvec = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing\n",
    "Now in the previous part we only defined our vectorizer to extract the data features but we will to this on the fly\n",
    "\n",
    "The next steps will be define a function called accuracy summary that is capable of test the accuracy of the model and measure the training time\n",
    "\n",
    "After that the nfeature accuracy checker will check the accuracy using  unigrams,bigrams and trigrams \n",
    "\n",
    "So first before to desing the NN i will use a simple linear regression and see which amount of features in the tfidf vectorizer is the best to use and the i will select the number of nodes that the NN needs based on this results.\n",
    "\n",
    "Finally we will compare the results using both data representation (word vectorizer and tfidvectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def decodeHotEncode(datum):\n",
    "    return np.argmax(datum)+1#because computers count from 0 and our clases starts in 1 :P\n",
    "\n",
    "def accuracy_summary(model, x_train, y_train, x_validation, y_validation,null_accuracy):\n",
    "    #label encoding\n",
    "    onehotencoder = OneHotEncoder(categories=\"auto\")\n",
    "    y_train_categorical=onehotencoder.fit_transform(y_train.values.reshape(-1, 1)).toarray()\n",
    "    y_validation_categorical=onehotencoder.transform(y_validation.values.reshape(-1, 1)).toarray()\n",
    "    #when is always guessing the most common class (null accuracy)\n",
    "    t0 = time()\n",
    "    print(f\"Train set:\\nx: {x_train.shape} \\ny: {y_train_categorical.shape}\")\n",
    "    print(f\"Validation set:\\nx: {x_validation.shape} \\ny: {y_validation_categorical.shape}\")\n",
    "    \n",
    "    model.fit_generator(generator=batch_generator(x_train, y_train_categorical, 32),\n",
    "                        epochs=5, validation_data=(x_validation, y_validation_categorical),\n",
    "                        steps_per_epoch=x_train.shape[0]/32)\n",
    "   \n",
    "    y_pred = model.predict(x_train)\n",
    "    #deconding the predictions\n",
    "    y_pred=[decodeHotEncode(i) for i in y_pred]\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_train, y_pred)\n",
    "    print(f\"{(y_train == y_pred).sum()} equal values:  of {y_train.shape[0]} samples\")\n",
    "    print (f\"Null accuracy: {(null_accuracy*100)}%\")\n",
    "    print (f\"Accuracy score: {(accuracy*100)}%\")\n",
    "    if accuracy > null_accuracy: # we have a good model :) (is guessing well the most part of time)\n",
    "        print (f\"The Model is {((accuracy-null_accuracy)*100)}% more accurate than null accuracy\")\n",
    "    elif accuracy == null_accuracy:# we have a  bad  model :( (is just guessing badly)\n",
    "        print (f\"The Model has the same accuracy with the null accuracy\")\n",
    "    else: # we have a very bad  model :'(  (is always confused with the most common class)\n",
    "        print (f\"The Model is {((null_accuracy-accuracy)*100)}% less accurate than null accuracy\")\n",
    "    print (f\"Train and Test time: {train_test_time}s\")\n",
    "    print (\"-\"*100)\n",
    "    return accuracy, train_test_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then lets desing our model:\n",
    " in order to know the better configuration of the NN i will check the acurracy amoung different   number of features. [10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]\n",
    " \n",
    "For now I will have  neurons in the input layer as many of the features  and 64 neurons in the  hidden layer with Relu activation function applied because in this case i want the neurons to be activated as much as its posible so Relu defined as f(x)=max(0,x) are always activated when the values are non negativ, also relu has a lot of advantages like : computacionaly cheao and also it avoids the  vanishing gradient problem.Also I'm using the adam optimizerfor updating the parameters and minimising the cost of the neural network. And the KL divergence loss function and also compare it whit categorical_crossentropy and finaly the output layer whit the sigmoid activation function\n",
    "\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before train train the model, Keras NN model cannot handle sparse matrix directly. The data has to be dense array or matrix, but transforming the whole training data Tfidf vectors of  to dense array won't fit into my RAM. So I had to define a function, which generates iterable generator object.The output should be a generator class object rather than arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[index_batch]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will check the acurracy for the following maxnumber the features:\n",
      " [  1000   2000   3000   4000   5000   6000   7000   8000   9000  10000\n",
      "  11000  12000  13000  14000  15000  16000  17000  18000  19000  20000\n",
      "  21000  22000  23000  24000  25000  26000  27000  28000  29000  30000\n",
      "  31000  32000  33000  34000  35000  36000  37000  38000  39000  40000\n",
      "  41000  42000  43000  44000  45000  46000  47000  48000  49000  50000\n",
      "  51000  52000  53000  54000  55000  56000  57000  58000  59000  60000\n",
      "  61000  62000  63000  64000  65000  66000  67000  68000  69000  70000\n",
      "  71000  72000  73000  74000  75000  76000  77000  78000  79000  80000\n",
      "  81000  82000  83000  84000  85000  86000  87000  88000  89000  90000\n",
      "  91000  92000  93000  94000  95000  96000  97000  98000  99000 100000]\n"
     ]
    }
   ],
   "source": [
    "n_features = np.arange(1000,100001,1000)\n",
    "\n",
    "print(\"We will check the acurracy for the following maxnumber the features:\\n\", n_features)\n",
    "import keras\n",
    "from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
    "def nfeature_accuracy_checker(x_train, y_train, x_validation, y_validation,null_accuracy,vectorizer=None, maxNumberOfFeatures=n_features, ngram_range=(1, 1)):\n",
    "    result = []\n",
    "    actualNumberofeatures=0\n",
    "    #feature extraction\n",
    "    for n in n_features:\n",
    "        keras.backend.clear_session()\n",
    "        vectorizer.set_params(max_features=n, ngram_range=ngram_range)\n",
    "        vectorizer.fit(x_train)\n",
    "        x_train_tfidf = tvec.transform(x_train)\n",
    "        x_validation_tfidf = tvec.transform(x_validation)\n",
    "        if(x_train_tfidf.shape[1]>actualNumberofeatures):\n",
    "            actualNumberofeatures=x_train_tfidf.shape[1]\n",
    "            print(\"Number of features obtained: \",x_train_tfidf.shape[1])    \n",
    "            classifier = Sequential()\n",
    "            classifier.add(Dense(64, activation='relu', input_dim=x_train_tfidf.shape[1]))\n",
    "            classifier.add(Dense(5, activation='sigmoid'))\n",
    "            classifier.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "        \n",
    "            print(\"Designed model\")\n",
    "            sequential_model_to_ascii_printout(classifier)\n",
    "            print (f\"\\nValidation result for {x_train_tfidf.shape[1]} features\")\n",
    "            nfeature_accuracy,tt_time = accuracy_summary(classifier, x_train_tfidf, y_train, x_validation_tfidf, y_validation,null_accuracy)\n",
    "            result.append((n,nfeature_accuracy,tt_time))\n",
    "        else:\n",
    "            print(\"the maximum number of features has been reached\")\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution:\n",
      "   sentiment  counts\n",
      "0          1      61\n",
      "1          2     378\n",
      "2          3    2629\n",
      "3          4     850\n",
      "4          5     282\n",
      "\n",
      "Validation set distribution:\n",
      "   sentiment  counts\n",
      "0          1      15\n",
      "1          2      95\n",
      "2          3     658\n",
      "3          4     212\n",
      "4          5      70\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 2000\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(training_data.text,training_data.sentiment, test_size=0.2, random_state=SEED,stratify=training_data.sentiment)\n",
    "print(\"Training set distribution:\")\n",
    "print(x_train.groupby(y_train).size().reset_index(name=\"counts\"))\n",
    "print(\"\\nValidation set distribution:\")\n",
    "print(x_validation.groupby(y_validation).size().reset_index(name=\"counts\"))\n",
    "null_accuracy=x_train.groupby(y_train).size().max()/x_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "***Results for unigram whit different number of features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features obtained:  1000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        1000\n",
      "               Dense   XXXXX -------------------     64064    99.5%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.5%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 1000 features\n",
      "Train set:\n",
      "x: (4200, 1000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 1000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 1s 7ms/step - loss: 1.3437 - acc: 0.6065 - val_loss: 1.0885 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 1s 6ms/step - loss: 1.0330 - acc: 0.6259 - val_loss: 1.0233 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 1s 5ms/step - loss: 0.9552 - acc: 0.6267 - val_loss: 0.9774 - val_acc: 0.6276\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 1s 5ms/step - loss: 0.8709 - acc: 0.6477 - val_loss: 0.9368 - val_acc: 0.6400\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 1s 5ms/step - loss: 0.7836 - acc: 0.6936 - val_loss: 0.9248 - val_acc: 0.6600\n",
      "3028 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 72.09523809523809%\n",
      "The Model is 9.499999999999996% more accurate than null accuracy\n",
      "Train and Test time: 4.215471029281616s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  2000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        2000\n",
      "               Dense   XXXXX -------------------    128064    99.7%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.3%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 2000 features\n",
      "Train set:\n",
      "x: (4200, 2000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 2000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 1s 9ms/step - loss: 1.3532 - acc: 0.6037 - val_loss: 1.1041 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 1s 7ms/step - loss: 1.0373 - acc: 0.6259 - val_loss: 1.0293 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 1s 8ms/step - loss: 0.9437 - acc: 0.6290 - val_loss: 0.9690 - val_acc: 0.6362\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 1s 7ms/step - loss: 0.8136 - acc: 0.6764 - val_loss: 0.9175 - val_acc: 0.6438\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 1s 7ms/step - loss: 0.6842 - acc: 0.7358 - val_loss: 0.9222 - val_acc: 0.6486\n",
      "3293 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 78.4047619047619%\n",
      "The Model is 15.809523809523807% more accurate than null accuracy\n",
      "Train and Test time: 5.567126750946045s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  3000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        3000\n",
      "               Dense   XXXXX -------------------    192064    99.8%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.2%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 3000 features\n",
      "Train set:\n",
      "x: (4200, 3000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 3000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 2s 12ms/step - loss: 1.3747 - acc: 0.6089 - val_loss: 1.1222 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 1s 11ms/step - loss: 1.0425 - acc: 0.6259 - val_loss: 1.0308 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 1s 9ms/step - loss: 0.9416 - acc: 0.6274 - val_loss: 0.9678 - val_acc: 0.6305\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 1s 11ms/step - loss: 0.7923 - acc: 0.6830 - val_loss: 0.9113 - val_acc: 0.6543\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 1s 9ms/step - loss: 0.6339 - acc: 0.7599 - val_loss: 0.9225 - val_acc: 0.6438\n",
      "3446 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 82.04761904761905%\n",
      "The Model is 19.452380952380956% more accurate than null accuracy\n",
      "Train and Test time: 7.473394870758057s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  4000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        4000\n",
      "               Dense   XXXXX -------------------    256064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 4000 features\n",
      "Train set:\n",
      "x: (4200, 4000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 4000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 2s 14ms/step - loss: 1.3728 - acc: 0.6158 - val_loss: 1.1099 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 2s 11ms/step - loss: 1.0289 - acc: 0.6259 - val_loss: 1.0210 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 2s 12ms/step - loss: 0.9179 - acc: 0.6309 - val_loss: 0.9580 - val_acc: 0.6419\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 2s 12ms/step - loss: 0.7549 - acc: 0.7079 - val_loss: 0.9206 - val_acc: 0.6429\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 2s 12ms/step - loss: 0.5908 - acc: 0.7893 - val_loss: 0.9413 - val_acc: 0.6333\n",
      "3585 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 85.35714285714285%\n",
      "The Model is 22.76190476190476% more accurate than null accuracy\n",
      "Train and Test time: 8.89488410949707s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  5000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        5000\n",
      "               Dense   XXXXX -------------------    320064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 5000 features\n",
      "Train set:\n",
      "x: (4200, 5000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 5000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 2s 17ms/step - loss: 1.3560 - acc: 0.6219 - val_loss: 1.1046 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 2s 15ms/step - loss: 1.0291 - acc: 0.6259 - val_loss: 1.0248 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 2s 14ms/step - loss: 0.9145 - acc: 0.6259 - val_loss: 0.9656 - val_acc: 0.6286\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 2s 14ms/step - loss: 0.7475 - acc: 0.6870 - val_loss: 0.9130 - val_acc: 0.6438\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 2s 13ms/step - loss: 0.5620 - acc: 0.8016 - val_loss: 0.9375 - val_acc: 0.6400\n",
      "3653 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 86.97619047619047%\n",
      "The Model is 24.38095238095238% more accurate than null accuracy\n",
      "Train and Test time: 10.613006830215454s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  6000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        6000\n",
      "               Dense   XXXXX -------------------    384064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 6000 features\n",
      "Train set:\n",
      "x: (4200, 6000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 6000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 3s 20ms/step - loss: 1.3459 - acc: 0.6200 - val_loss: 1.0970 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 2s 17ms/step - loss: 1.0279 - acc: 0.6259 - val_loss: 1.0264 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 2s 15ms/step - loss: 0.9108 - acc: 0.6293 - val_loss: 0.9622 - val_acc: 0.6362\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 2s 15ms/step - loss: 0.7251 - acc: 0.7126 - val_loss: 0.9196 - val_acc: 0.6448\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 2s 15ms/step - loss: 0.5299 - acc: 0.8210 - val_loss: 0.9485 - val_acc: 0.6371\n",
      "3739 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 89.02380952380953%\n",
      "The Model is 26.428571428571434% more accurate than null accuracy\n",
      "Train and Test time: 11.712377786636353s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  7000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        7000\n",
      "               Dense   XXXXX -------------------    448064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 7000 features\n",
      "Train set:\n",
      "x: (4200, 7000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 7000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 3s 20ms/step - loss: 1.3494 - acc: 0.6181 - val_loss: 1.1091 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 2s 15ms/step - loss: 1.0411 - acc: 0.6259 - val_loss: 1.0382 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 2s 15ms/step - loss: 0.9363 - acc: 0.6259 - val_loss: 0.9761 - val_acc: 0.6276\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 2s 15ms/step - loss: 0.7554 - acc: 0.6759 - val_loss: 0.9128 - val_acc: 0.6457\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 2s 15ms/step - loss: 0.5331 - acc: 0.8160 - val_loss: 0.9363 - val_acc: 0.6390\n",
      "3749 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 89.26190476190476%\n",
      "The Model is 26.66666666666667% more accurate than null accuracy\n",
      "Train and Test time: 11.623130798339844s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  8000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        8000\n",
      "               Dense   XXXXX -------------------    512064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 8000 features\n",
      "Train set:\n",
      "x: (4200, 8000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 8000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 3s 21ms/step - loss: 1.3736 - acc: 0.6132 - val_loss: 1.1163 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 3s 19ms/step - loss: 1.0333 - acc: 0.6259 - val_loss: 1.0236 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 2s 17ms/step - loss: 0.8994 - acc: 0.6293 - val_loss: 0.9531 - val_acc: 0.6400\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 3s 22ms/step - loss: 0.6870 - acc: 0.7277 - val_loss: 0.9190 - val_acc: 0.6419\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 3s 22ms/step - loss: 0.4758 - acc: 0.8565 - val_loss: 0.9567 - val_acc: 0.6352\n",
      "3859 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 91.88095238095238%\n",
      "The Model is 29.285714285714292% more accurate than null accuracy\n",
      "Train and Test time: 14.43820309638977s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  8541\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        8541\n",
      "               Dense   XXXXX -------------------    546688    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 8541 features\n",
      "Train set:\n",
      "x: (4200, 8541) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 8541) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 3s 21ms/step - loss: 1.3633 - acc: 0.6139 - val_loss: 1.1048 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 2s 18ms/step - loss: 1.0274 - acc: 0.6259 - val_loss: 1.0214 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 3s 19ms/step - loss: 0.8931 - acc: 0.6295 - val_loss: 0.9550 - val_acc: 0.6390\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 3s 20ms/step - loss: 0.6796 - acc: 0.7322 - val_loss: 0.9179 - val_acc: 0.6419\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 3s 19ms/step - loss: 0.4619 - acc: 0.8551 - val_loss: 0.9519 - val_acc: 0.6343\n",
      "3854 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 91.76190476190477%\n",
      "The Model is 29.166666666666675% more accurate than null accuracy\n",
      "Train and Test time: 14.077871084213257s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "the maximum number of features has been reached\n"
     ]
    }
   ],
   "source": [
    "feature_result_ugt = nfeature_accuracy_checker(x_train,y_train,x_validation,y_validation,null_accuracy,vectorizer=tvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1000, 0.7209523809523809, 4.215471029281616), (2000, 0.784047619047619, 5.567126750946045), (3000, 0.8204761904761905, 7.473394870758057), (4000, 0.8535714285714285, 8.89488410949707), (5000, 0.8697619047619047, 10.613006830215454), (6000, 0.8902380952380953, 11.712377786636353), (7000, 0.8926190476190476, 11.623130798339844), (8000, 0.9188095238095239, 14.43820309638977), (9000, 0.9176190476190477, 14.077871084213257)]\n"
     ]
    }
   ],
   "source": [
    "print(feature_result_ugt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Results for Bigram whit different number of features***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features obtained:  1000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        1000\n",
      "               Dense   XXXXX -------------------     64064    99.5%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.5%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 1000 features\n",
      "Train set:\n",
      "x: (4200, 1000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 1000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 1s 7ms/step - loss: 1.3344 - acc: 0.5975 - val_loss: 1.0913 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 1s 5ms/step - loss: 1.0399 - acc: 0.6259 - val_loss: 1.0305 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 1s 5ms/step - loss: 0.9659 - acc: 0.6295 - val_loss: 0.9836 - val_acc: 0.6343\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 1s 5ms/step - loss: 0.8750 - acc: 0.6600 - val_loss: 0.9448 - val_acc: 0.6486\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 1s 6ms/step - loss: 0.7898 - acc: 0.6939 - val_loss: 0.9442 - val_acc: 0.6438\n",
      "3027 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 72.07142857142857%\n",
      "The Model is 9.476190476190482% more accurate than null accuracy\n",
      "Train and Test time: 4.4236578941345215s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  2000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        2000\n",
      "               Dense   XXXXX -------------------    128064    99.7%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.3%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 2000 features\n",
      "Train set:\n",
      "x: (4200, 2000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 2000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 1s 10ms/step - loss: 1.3589 - acc: 0.6096 - val_loss: 1.0978 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 1s 8ms/step - loss: 1.0210 - acc: 0.6259 - val_loss: 1.0116 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 1s 7ms/step - loss: 0.9202 - acc: 0.6295 - val_loss: 0.9625 - val_acc: 0.6352\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 1s 7ms/step - loss: 0.8022 - acc: 0.6771 - val_loss: 0.9298 - val_acc: 0.6600\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 1s 8ms/step - loss: 0.6852 - acc: 0.7358 - val_loss: 0.9446 - val_acc: 0.6619\n",
      "3277 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 78.02380952380953%\n",
      "The Model is 15.428571428571436% more accurate than null accuracy\n",
      "Train and Test time: 6.231271028518677s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  3000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        3000\n",
      "               Dense   XXXXX -------------------    192064    99.8%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.2%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 3000 features\n",
      "Train set:\n",
      "x: (4200, 3000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 3000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 2s 13ms/step - loss: 1.3319 - acc: 0.6143 - val_loss: 1.0921 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 1s 10ms/step - loss: 1.0265 - acc: 0.6259 - val_loss: 1.0208 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 1s 11ms/step - loss: 0.9270 - acc: 0.6304 - val_loss: 0.9663 - val_acc: 0.6390\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 1s 10ms/step - loss: 0.7875 - acc: 0.6906 - val_loss: 0.9291 - val_acc: 0.6581\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 1s 9ms/step - loss: 0.6428 - acc: 0.7583 - val_loss: 0.9506 - val_acc: 0.6495\n",
      "3442 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 81.95238095238095%\n",
      "The Model is 19.35714285714286% more accurate than null accuracy\n",
      "Train and Test time: 7.948617935180664s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  4000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        4000\n",
      "               Dense   XXXXX -------------------    256064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 4000 features\n",
      "Train set:\n",
      "x: (4200, 4000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 4000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 2s 13ms/step - loss: 1.3554 - acc: 0.6184 - val_loss: 1.1128 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 1s 11ms/step - loss: 1.0277 - acc: 0.6259 - val_loss: 1.0239 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 1s 11ms/step - loss: 0.9155 - acc: 0.6276 - val_loss: 0.9686 - val_acc: 0.6314\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 1s 10ms/step - loss: 0.7601 - acc: 0.6828 - val_loss: 0.9224 - val_acc: 0.6590\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 1s 11ms/step - loss: 0.5853 - acc: 0.7793 - val_loss: 0.9512 - val_acc: 0.6467\n",
      "3595 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 85.59523809523809%\n",
      "The Model is 23.0% more accurate than null accuracy\n",
      "Train and Test time: 8.357043027877808s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  5000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        5000\n",
      "               Dense   XXXXX -------------------    320064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 5000 features\n",
      "Train set:\n",
      "x: (4200, 5000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 5000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 2s 17ms/step - loss: 1.3407 - acc: 0.6160 - val_loss: 1.0960 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 2s 13ms/step - loss: 1.0196 - acc: 0.6259 - val_loss: 1.0147 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 2s 13ms/step - loss: 0.8765 - acc: 0.6501 - val_loss: 0.9412 - val_acc: 0.6533\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 2s 13ms/step - loss: 0.6723 - acc: 0.7464 - val_loss: 0.9361 - val_acc: 0.6448\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 2s 14ms/step - loss: 0.5042 - acc: 0.8236 - val_loss: 0.9787 - val_acc: 0.6476\n",
      "3756 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 89.42857142857143%\n",
      "The Model is 26.833333333333332% more accurate than null accuracy\n",
      "Train and Test time: 10.03007197380066s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  6000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        6000\n",
      "               Dense   XXXXX -------------------    384064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 6000 features\n",
      "Train set:\n",
      "x: (4200, 6000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 6000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 2s 18ms/step - loss: 1.3468 - acc: 0.6205 - val_loss: 1.0909 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 2s 14ms/step - loss: 1.0046 - acc: 0.6259 - val_loss: 1.0094 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 2s 14ms/step - loss: 0.8710 - acc: 0.6302 - val_loss: 0.9555 - val_acc: 0.6352\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 2s 14ms/step - loss: 0.6979 - acc: 0.7057 - val_loss: 0.9208 - val_acc: 0.6505\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 2s 14ms/step - loss: 0.5125 - acc: 0.8134 - val_loss: 0.9531 - val_acc: 0.6495\n",
      "3727 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 88.73809523809524%\n",
      "The Model is 26.142857142857146% more accurate than null accuracy\n",
      "Train and Test time: 10.694356918334961s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  7000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        7000\n",
      "               Dense   XXXXX -------------------    448064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 7000 features\n",
      "Train set:\n",
      "x: (4200, 7000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 7000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 2s 19ms/step - loss: 1.3381 - acc: 0.6233 - val_loss: 1.0905 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 2s 16ms/step - loss: 1.0118 - acc: 0.6259 - val_loss: 1.0130 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 2s 16ms/step - loss: 0.8656 - acc: 0.6404 - val_loss: 0.9429 - val_acc: 0.6448\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 2s 16ms/step - loss: 0.6474 - acc: 0.7535 - val_loss: 0.9332 - val_acc: 0.6467\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 2s 16ms/step - loss: 0.4555 - acc: 0.8518 - val_loss: 0.9772 - val_acc: 0.6429\n",
      "3862 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 91.95238095238095%\n",
      "The Model is 29.35714285714286% more accurate than null accuracy\n",
      "Train and Test time: 12.060122966766357s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  8000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        8000\n",
      "               Dense   XXXXX -------------------    512064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 8000 features\n",
      "Train set:\n",
      "x: (4200, 8000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 8000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 3s 22ms/step - loss: 1.3208 - acc: 0.6236 - val_loss: 1.0838 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 2s 18ms/step - loss: 1.0023 - acc: 0.6259 - val_loss: 1.0121 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 2s 18ms/step - loss: 0.8555 - acc: 0.6342 - val_loss: 0.9441 - val_acc: 0.6429\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 2s 18ms/step - loss: 0.6294 - acc: 0.7471 - val_loss: 0.9262 - val_acc: 0.6562\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 2s 18ms/step - loss: 0.4255 - acc: 0.8667 - val_loss: 0.9772 - val_acc: 0.6438\n",
      "3934 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 93.66666666666667%\n",
      "The Model is 31.071428571428573% more accurate than null accuracy\n",
      "Train and Test time: 13.570160865783691s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  9000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####        9000\n",
      "               Dense   XXXXX -------------------    576064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 9000 features\n",
      "Train set:\n",
      "x: (4200, 9000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 9000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 3s 23ms/step - loss: 1.3649 - acc: 0.6188 - val_loss: 1.1240 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 3s 20ms/step - loss: 1.0277 - acc: 0.6259 - val_loss: 1.0196 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 3s 20ms/step - loss: 0.8648 - acc: 0.6449 - val_loss: 0.9405 - val_acc: 0.6476\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 3s 20ms/step - loss: 0.6119 - acc: 0.7718 - val_loss: 0.9399 - val_acc: 0.6467\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 3s 20ms/step - loss: 0.3997 - acc: 0.8814 - val_loss: 0.9962 - val_acc: 0.6381\n",
      "3972 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 94.57142857142857%\n",
      "The Model is 31.97619047619048% more accurate than null accuracy\n",
      "Train and Test time: 14.813899993896484s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  10000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       10000\n",
      "               Dense   XXXXX -------------------    640064    99.9%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.1%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 10000 features\n",
      "Train set:\n",
      "x: (4200, 10000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 10000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 3s 24ms/step - loss: 1.3585 - acc: 0.6196 - val_loss: 1.1043 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 3s 22ms/step - loss: 1.0090 - acc: 0.6259 - val_loss: 1.0158 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 3s 22ms/step - loss: 0.8427 - acc: 0.6392 - val_loss: 0.9433 - val_acc: 0.6419\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 3s 22ms/step - loss: 0.5915 - acc: 0.7827 - val_loss: 0.9415 - val_acc: 0.6429\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 3s 22ms/step - loss: 0.3827 - acc: 0.8901 - val_loss: 0.9962 - val_acc: 0.6324\n",
      "3997 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 95.16666666666667%\n",
      "The Model is 32.57142857142858% more accurate than null accuracy\n",
      "Train and Test time: 15.888775825500488s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  11000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       11000\n",
      "               Dense   XXXXX -------------------    704064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 11000 features\n",
      "Train set:\n",
      "x: (4200, 11000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 11000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 4s 28ms/step - loss: 1.3823 - acc: 0.6122 - val_loss: 1.1350 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 3s 24ms/step - loss: 1.0361 - acc: 0.6259 - val_loss: 1.0286 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 3s 24ms/step - loss: 0.8758 - acc: 0.6366 - val_loss: 0.9499 - val_acc: 0.6438\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 3s 24ms/step - loss: 0.6037 - acc: 0.7777 - val_loss: 0.9464 - val_acc: 0.6457\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 3s 23ms/step - loss: 0.3708 - acc: 0.8977 - val_loss: 1.0061 - val_acc: 0.6371\n",
      "4033 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 96.02380952380952%\n",
      "The Model is 33.42857142857143% more accurate than null accuracy\n",
      "Train and Test time: 17.499263048171997s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  12000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       12000\n",
      "               Dense   XXXXX -------------------    768064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 12000 features\n",
      "Train set:\n",
      "x: (4200, 12000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 12000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 4s 28ms/step - loss: 1.3476 - acc: 0.6153 - val_loss: 1.1008 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 3s 24ms/step - loss: 1.0045 - acc: 0.6259 - val_loss: 1.0120 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 3s 24ms/step - loss: 0.8226 - acc: 0.6527 - val_loss: 0.9372 - val_acc: 0.6476\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 3s 25ms/step - loss: 0.5530 - acc: 0.8068 - val_loss: 0.9519 - val_acc: 0.6505\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 3s 25ms/step - loss: 0.3450 - acc: 0.9107 - val_loss: 1.0093 - val_acc: 0.6362\n",
      "4030 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 95.95238095238095%\n",
      "The Model is 33.35714285714286% more accurate than null accuracy\n",
      "Train and Test time: 17.778898000717163s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  13000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       13000\n",
      "               Dense   XXXXX -------------------    832064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 13000 features\n",
      "Train set:\n",
      "x: (4200, 13000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 13000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 4s 30ms/step - loss: 1.3515 - acc: 0.6188 - val_loss: 1.0806 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 3s 26ms/step - loss: 0.9892 - acc: 0.6259 - val_loss: 1.0032 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 3s 26ms/step - loss: 0.8053 - acc: 0.6508 - val_loss: 0.9340 - val_acc: 0.6419\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 4s 27ms/step - loss: 0.5468 - acc: 0.8016 - val_loss: 0.9400 - val_acc: 0.6505\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 4s 27ms/step - loss: 0.3398 - acc: 0.9119 - val_loss: 0.9940 - val_acc: 0.6429\n",
      "4041 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 96.21428571428572%\n",
      "The Model is 33.61904761904763% more accurate than null accuracy\n",
      "Train and Test time: 19.08660101890564s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  14000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       14000\n",
      "               Dense   XXXXX -------------------    896064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 14000 features\n",
      "Train set:\n",
      "x: (4200, 14000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 14000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 4s 31ms/step - loss: 1.3564 - acc: 0.6186 - val_loss: 1.0935 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 4s 28ms/step - loss: 1.0046 - acc: 0.6259 - val_loss: 1.0143 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 4s 28ms/step - loss: 0.8366 - acc: 0.6461 - val_loss: 0.9420 - val_acc: 0.6419\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 4s 28ms/step - loss: 0.5668 - acc: 0.7957 - val_loss: 0.9439 - val_acc: 0.6476\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 4s 29ms/step - loss: 0.3408 - acc: 0.9081 - val_loss: 0.9964 - val_acc: 0.6362\n",
      "4055 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 96.54761904761905%\n",
      "The Model is 33.952380952380956% more accurate than null accuracy\n",
      "Train and Test time: 20.32566809654236s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  15000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       15000\n",
      "               Dense   XXXXX -------------------    960064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 15000 features\n",
      "Train set:\n",
      "x: (4200, 15000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 15000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 4s 33ms/step - loss: 1.3973 - acc: 0.6080 - val_loss: 1.1265 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 4s 30ms/step - loss: 1.0095 - acc: 0.6259 - val_loss: 1.0086 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 4s 31ms/step - loss: 0.8035 - acc: 0.6586 - val_loss: 0.9310 - val_acc: 0.6457\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 4s 30ms/step - loss: 0.5048 - acc: 0.8407 - val_loss: 0.9521 - val_acc: 0.6438\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 4s 30ms/step - loss: 0.2954 - acc: 0.9325 - val_loss: 1.0090 - val_acc: 0.6295\n",
      "4075 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 97.02380952380952%\n",
      "The Model is 34.42857142857143% more accurate than null accuracy\n",
      "Train and Test time: 21.485475063323975s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  16000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       16000\n",
      "               Dense   XXXXX -------------------   1024064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 16000 features\n",
      "Train set:\n",
      "x: (4200, 16000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 16000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 5s 35ms/step - loss: 1.3775 - acc: 0.6151 - val_loss: 1.1186 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 4s 32ms/step - loss: 1.0066 - acc: 0.6259 - val_loss: 1.0096 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 4s 33ms/step - loss: 0.8028 - acc: 0.6643 - val_loss: 0.9319 - val_acc: 0.6562\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 4s 32ms/step - loss: 0.5017 - acc: 0.8326 - val_loss: 0.9593 - val_acc: 0.6448\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 4s 32ms/step - loss: 0.2930 - acc: 0.9278 - val_loss: 1.0156 - val_acc: 0.6295\n",
      "4071 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 96.92857142857143%\n",
      "The Model is 34.333333333333336% more accurate than null accuracy\n",
      "Train and Test time: 22.86473798751831s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  17000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       17000\n",
      "               Dense   XXXXX -------------------   1088064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 17000 features\n",
      "Train set:\n",
      "x: (4200, 17000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 17000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 5s 37ms/step - loss: 1.3576 - acc: 0.6167 - val_loss: 1.1007 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 5s 35ms/step - loss: 1.0038 - acc: 0.6259 - val_loss: 1.0133 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 5s 35ms/step - loss: 0.8017 - acc: 0.6674 - val_loss: 0.9380 - val_acc: 0.6543\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 5s 35ms/step - loss: 0.4903 - acc: 0.8508 - val_loss: 0.9682 - val_acc: 0.6400\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 5s 34ms/step - loss: 0.2773 - acc: 0.9366 - val_loss: 1.0221 - val_acc: 0.6352\n",
      "4094 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 97.47619047619047%\n",
      "The Model is 34.88095238095238% more accurate than null accuracy\n",
      "Train and Test time: 24.54235291481018s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  18000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       18000\n",
      "               Dense   XXXXX -------------------   1152064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 18000 features\n",
      "Train set:\n",
      "x: (4200, 18000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 18000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 5s 38ms/step - loss: 1.3741 - acc: 0.6181 - val_loss: 1.1133 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 5s 35ms/step - loss: 1.0124 - acc: 0.6259 - val_loss: 1.0141 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 5s 35ms/step - loss: 0.8040 - acc: 0.6700 - val_loss: 0.9309 - val_acc: 0.6514\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 5s 35ms/step - loss: 0.4868 - acc: 0.8482 - val_loss: 0.9533 - val_acc: 0.6438\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 5s 35ms/step - loss: 0.2758 - acc: 0.9377 - val_loss: 1.0042 - val_acc: 0.6371\n",
      "4087 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 97.30952380952381%\n",
      "The Model is 34.71428571428572% more accurate than null accuracy\n",
      "Train and Test time: 24.71198606491089s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  19000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       19000\n",
      "               Dense   XXXXX -------------------   1216064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 19000 features\n",
      "Train set:\n",
      "x: (4200, 19000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 19000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 6s 42ms/step - loss: 1.3675 - acc: 0.6160 - val_loss: 1.1155 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 5s 37ms/step - loss: 1.0143 - acc: 0.6259 - val_loss: 1.0196 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 5s 37ms/step - loss: 0.8238 - acc: 0.6449 - val_loss: 0.9383 - val_acc: 0.6448\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 5s 37ms/step - loss: 0.5090 - acc: 0.8371 - val_loss: 0.9516 - val_acc: 0.6419\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 5s 37ms/step - loss: 0.2708 - acc: 0.9408 - val_loss: 1.0064 - val_acc: 0.6324\n",
      "4106 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 97.76190476190476%\n",
      "The Model is 35.16666666666667% more accurate than null accuracy\n",
      "Train and Test time: 26.587326049804688s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  20000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       20000\n",
      "               Dense   XXXXX -------------------   1280064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 20000 features\n",
      "Train set:\n",
      "x: (4200, 20000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 20000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 5s 41ms/step - loss: 1.3601 - acc: 0.6203 - val_loss: 1.1029 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 5s 38ms/step - loss: 0.9999 - acc: 0.6259 - val_loss: 1.0173 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 5s 38ms/step - loss: 0.8094 - acc: 0.6501 - val_loss: 0.9403 - val_acc: 0.6457\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 5s 38ms/step - loss: 0.4948 - acc: 0.8516 - val_loss: 0.9493 - val_acc: 0.6467\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 5s 38ms/step - loss: 0.2682 - acc: 0.9444 - val_loss: 0.9982 - val_acc: 0.6390\n",
      "4104 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 97.71428571428571%\n",
      "The Model is 35.11904761904761% more accurate than null accuracy\n",
      "Train and Test time: 26.984176874160767s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  21000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       21000\n",
      "               Dense   XXXXX -------------------   1344064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 21000 features\n",
      "Train set:\n",
      "x: (4200, 21000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 21000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 6s 44ms/step - loss: 1.3658 - acc: 0.6172 - val_loss: 1.1040 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 5s 41ms/step - loss: 1.0060 - acc: 0.6259 - val_loss: 1.0153 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 5s 41ms/step - loss: 0.8026 - acc: 0.6600 - val_loss: 0.9344 - val_acc: 0.6590\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 5s 41ms/step - loss: 0.4831 - acc: 0.8343 - val_loss: 0.9565 - val_acc: 0.6429\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 5s 41ms/step - loss: 0.2647 - acc: 0.9375 - val_loss: 1.0046 - val_acc: 0.6448\n",
      "4102 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 97.66666666666667%\n",
      "The Model is 35.07142857142858% more accurate than null accuracy\n",
      "Train and Test time: 29.305548191070557s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  22000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       22000\n",
      "               Dense   XXXXX -------------------   1408064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 22000 features\n",
      "Train set:\n",
      "x: (4200, 22000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 22000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 6s 47ms/step - loss: 1.3507 - acc: 0.6212 - val_loss: 1.0874 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 6s 43ms/step - loss: 0.9884 - acc: 0.6259 - val_loss: 1.0102 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 6s 42ms/step - loss: 0.7651 - acc: 0.6802 - val_loss: 0.9418 - val_acc: 0.6467\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 6s 46ms/step - loss: 0.4514 - acc: 0.8726 - val_loss: 0.9692 - val_acc: 0.6400\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 6s 49ms/step - loss: 0.2447 - acc: 0.9553 - val_loss: 1.0178 - val_acc: 0.6314\n",
      "4126 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 98.23809523809524%\n",
      "The Model is 35.642857142857146% more accurate than null accuracy\n",
      "Train and Test time: 31.731029987335205s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  23000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       23000\n",
      "               Dense   XXXXX -------------------   1472064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 23000 features\n",
      "Train set:\n",
      "x: (4200, 23000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 23000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 7s 49ms/step - loss: 1.3479 - acc: 0.6226 - val_loss: 1.0831 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 6s 45ms/step - loss: 0.9859 - acc: 0.6259 - val_loss: 1.0095 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 6s 45ms/step - loss: 0.7736 - acc: 0.6579 - val_loss: 0.9318 - val_acc: 0.6505\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 6s 44ms/step - loss: 0.4461 - acc: 0.8707 - val_loss: 0.9484 - val_acc: 0.6410\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 6s 45ms/step - loss: 0.2291 - acc: 0.9562 - val_loss: 0.9997 - val_acc: 0.6362\n",
      "4138 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 98.52380952380952%\n",
      "The Model is 35.92857142857143% more accurate than null accuracy\n",
      "Train and Test time: 31.98490309715271s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  24000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       24000\n",
      "               Dense   XXXXX -------------------   1536064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 24000 features\n",
      "Train set:\n",
      "x: (4200, 24000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 24000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 6s 48ms/step - loss: 1.3620 - acc: 0.6148 - val_loss: 1.1085 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      "132/131 [==============================] - 6s 46ms/step - loss: 1.0119 - acc: 0.6259 - val_loss: 1.0167 - val_acc: 0.6267\n",
      "Epoch 3/5\n",
      "132/131 [==============================] - 6s 46ms/step - loss: 0.8034 - acc: 0.6572 - val_loss: 0.9310 - val_acc: 0.6552\n",
      "Epoch 4/5\n",
      "132/131 [==============================] - 6s 46ms/step - loss: 0.4648 - acc: 0.8492 - val_loss: 0.9580 - val_acc: 0.6486\n",
      "Epoch 5/5\n",
      "132/131 [==============================] - 6s 46ms/step - loss: 0.2396 - acc: 0.9498 - val_loss: 1.0082 - val_acc: 0.6381\n",
      "4127 equal values:  of 4200 samples\n",
      "Null accuracy: 62.595238095238095%\n",
      "Accuracy score: 98.26190476190476%\n",
      "The Model is 35.66666666666667% more accurate than null accuracy\n",
      "Train and Test time: 32.434985876083374s\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Number of features obtained:  25000\n",
      "Designed model\n",
      "           OPERATION           DATA DIMENSIONS   WEIGHTS(N)   WEIGHTS(%)\n",
      "\n",
      "               Input   #####       25000\n",
      "               Dense   XXXXX -------------------   1600064   100.0%\n",
      "                relu   #####          64\n",
      "               Dense   XXXXX -------------------       325     0.0%\n",
      "             sigmoid   #####           5\n",
      "\n",
      "Validation result for 25000 features\n",
      "Train set:\n",
      "x: (4200, 25000) \n",
      "y: (4200, 5)\n",
      "Validation set:\n",
      "x: (1050, 25000) \n",
      "y: (1050, 5)\n",
      "Epoch 1/5\n",
      "132/131 [==============================] - 7s 50ms/step - loss: 1.3590 - acc: 0.6153 - val_loss: 1.0949 - val_acc: 0.6267\n",
      "Epoch 2/5\n",
      " 81/131 [=================>............] - ETA: 2s - loss: 1.0158 - acc: 0.6285"
     ]
    }
   ],
   "source": [
    "feature_result_bgt = nfeature_accuracy_checker(x_train,y_train,x_validation,y_validation,null_accuracy,vectorizer=tvec,ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SentimentAnalysis)",
   "language": "python",
   "name": "sentimentanalysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
